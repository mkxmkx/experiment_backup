# Word embeddings.

glove_300d_filtered {
  #path = /home/makexin/makexin/Experiment/data/v4/Global_warming/glove.840B.300d.txt.filtered
  path = /home/makexin/makexin/Experiment/data/v2/glove.840B.300d.txt.filtered
  size = 300
}

glove_300d_2w {
  path = /home/makexin/makexin/Experiment/data/glove_50_300_2.txt
  size = 300
}


# Distributed training configurations.
# GPU指定
two_local_gpus {
  addresses {
    ps = [localhost:7222]
    worker = [localhost:7222, localhost:7222]
  }
  gpus = [0, 1, 2, 3]
}

# Main configuration.
best {
  # Computation limits.
  max_top_antecedents = 50    #每个span，最多的先行词
  max_training_sentences = 50   #一次训练最多的训练句子数量
  top_span_ratio = 0.4
  top_result_ratio = 0.4   #最终结果，取antecedent的前百分比

  # Model hyperparameters.  模型参数
  filter_widths = [3, 4, 5]     #filter_widths的用处？？？？
  filter_size = 50
  char_embedding_size = 8    #没有char embedding
  char_vocab_path = "/home/makexin/makexin/Experiment/data/v1/char_vocab.txt"   #句子集合
  head_embeddings = ${glove_300d_2w}       #head embedding也没有
  context_embeddings = ${glove_300d_filtered}   #文本embedding
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 160
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30   #最长span宽度
  use_metadata = false   #不使用metadata，因为没有
  use_features = true                                  #改为false，因为get_span_emb里面考虑特征的话维度不统一，暂时改为false
  model_heads = true   #需修改
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true
  lambda = 0.01   #初始为0.01
  related_metric = 0.0  # 用于判断topic和span是否相关的评价阈值
  result_metric = 0.3  #用于判断最终两个topic是否有先后序关系的评价阈值

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = /home/makexin/makexin/Experiment/data/v2/train_topic_pair_sentences_train_data.json       #训练数据、测试数据文本路径
  eval_path = /home/makexin/makexin/Experiment/data/v2/eval_topic_pair_sentences_train_data.json                 #测试数据路径
  eval_result_file = /home/makexin/makexin/Experiment/data/v2/eval_result.txt
  lm_path = /home/makexin/makexin/Experiment/data/v2/elmo_cache.hdf5                      #elmo词向量
  #genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
  cluster = ${two_local_gpus}
}

# For evaluation. Do not use for training (i.e. only for predict.py, evaluate.py, and demo.py). Rename `best` directory to `final`.
final = ${best} {
  context_embeddings = ${glove_300d_filtered}
  #head_embeddings = ${glove_300d_2w}
  lm_path = ""
  #eval_path = /home/makexin/makexin/Experiment/data/v4/Global_warming/eval_topic_pair_sentences_train_data.json           #训练数据文本路径
  eval_path = /home/makexin/makexin/Experiment/data/v2/eval_topic_pair_sentences_train_data.json
  #conll_eval_path = test.english.v4_gold_conll
}

# Baselines.
c2f_100_ant = ${best} {
  max_top_antecedents = 100
}
c2f_250_ant = ${best} {
  max_top_antecedents = 250
}
c2f_1_layer = ${best} {
  coref_depth = 1
}
c2f_3_layer = ${best} {
  coref_depth = 3
}
distance_50_ant = ${best} {
  max_top_antecedents = 50
  coarse_to_fine = false
  coref_depth = 1
}
distance_100_ant = ${distance_50_ant} {
  max_top_antecedents = 100
}
distance_250_ant = ${distance_50_ant} {
  max_top_antecedents = 250
}
